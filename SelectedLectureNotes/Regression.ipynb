{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Linear Regression](#linreg)\n",
    "- [Least Squares Algorithm and its Derivation](#LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linreg\"></a>\n",
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Regression - just two varialbles, one which is the explanatory variable (x) and the other one is response variable (y). This type of regression can be easily illustrated with scatterplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Correlation Coefficient (r)</b> - the strength and direction of a linear relationship. $r \\in [-1,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundaries for the strengh of correlation depend on the fiels. General guidelines:  \n",
    "- Strong: $0.7 \\leq |r| < 1.0$   \n",
    "- Moderate: $0.3 \\leq |r| < 0.7$   \n",
    "- Weak: $0.0 \\leq |r| < 0.3$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) } { {\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} {\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important:</b> if $r=0$ it DOES NOT necessarily mean that there is no relationship at all. It just means that there is no <b>linear</b> relationships. So, correlation cofficient only captures linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('quizzes-data-1.xlsx')\n",
    "df = df[df.Temp.notnull()&df.Sales.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.95902026],\n",
       "       [0.95902026, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df.Temp, df.Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b_0$: The **intercept** is defined as the **predicted value of the response when the x-variable is zero**.\n",
    "\n",
    "$b_i, i \\geq 1$ : The **slope**: for every unit increase in x **the expected increase in y by the slope, holding all else (other variables) constant**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression equation is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = b_0 + b_1 x_1 + ... + b_n x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b_0, b_1, ... ,b_n$ are statistic values, whereas $\\beta_0, \\beta_1,..., \\beta_n$ are actual, population parameters. Also, $\\hat{y}$ is predicted value, whereas $y$ is actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"lsa\"></a>\n",
    "# Least Squares Algorithm  and its Derviation\n",
    "Goal: Minimize the sum of the squared vertical distances from the line to points. Objective function will look like this:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = \\sum_{i=1}^n (y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other loss functions are possible, but this one is the easiest one to work with since it's easy to take it's derivative which is necessary for finding the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our $\\hat{y}^{(i)}$ as:\n",
    "$$(x^{(i)})^T b$$, \n",
    "where $x^{(i)}$ is a vector $[1; {x_{o}^{(i)}}]$ (I'm turning original scalar $x_{o}^{(i)}$ to a vector so that we can pack it into a dot product and then to matrix-vector multiplication. And for the sake of generalisation let's actually make $x^{(i)}$ n-dimensional. I.e. our i-th observation has n features.   \n",
    "By default, all vectors are column vectors. Now, b is a vector $(b_0, b_1, ... b_n)$. Check that we get the same result after these arrangements: \n",
    "\n",
    "$$(x^{(i)})^T b = [1; x_1,...,x_n]^T [b_0, b_1,.. b_n] = b_0 + b_1 x^{(i)}_1 + b_2 x^{(i)}_2 + ... + b_n x^{(i)}_n $$\n",
    "\n",
    "So we can rewrite our objective function as: \n",
    "\n",
    "$$E(b) = \\sum_{i=1}^{n} (y^{(i)} - (x^{(i)})^T b)^2$$ \n",
    "\n",
    "This sum is actually the definiton of a dot product, so we can further rewrite it as: \n",
    "\n",
    "$$E(b) = \\sum_{i=1}^{n} (y^{(i)} - (x^{(i)})^T b)^2 = (y-Xb)^T (y-Xb)$$, \n",
    "\n",
    "where X is a n by 2 matrix with the first column being all 1s. So when we multiply this matrix by vector b, we'll get $\\hat{y}$ vector of predictions. Now we can minimize this function, but first we will expand it: \n",
    "\n",
    "$$E(b) = (y-Xb)^T (y-Xb) = y^T y - y^T X b - b^T X^T y + b^T X^T X b$$\n",
    "\n",
    "Here, it's important to notice that $y^T X b = b^T X^T y$, so we can now write: \n",
    "\n",
    "$$E(b) = y^T y - 2 b^T X^T y + b^T X^T X b $$\n",
    "\n",
    "And now we will take the derivative of this guy and equate it to $\\vec{0}$:  \n",
    "\n",
    "$$\\nabla{E} = - 2 X^T y + 2 X^T X b = \\vec{0} $$\n",
    "\n",
    "And now we can find the b vector as follows: \n",
    "\n",
    "$$X^T y = X^T X b$$\n",
    "\n",
    "$$b = (X^T X)^{-1} X^T y $$  \n",
    "\n",
    "The only possible problem here is that the matrix might appear to be non-invertible and in this case there are special techniques that help to avoid it. Typically, pseudoinverse is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/house_prices.csv')\n",
    "# add ones column\n",
    "df['intercept'] = 1 \n",
    "X = df[['intercept', 'area', 'bathrooms', 'bedrooms']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.dot(np.dot(np.linalg.pinv(np.dot(X.transpose(),X)), X.transpose()),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10072.10704941,   345.91101884,  7345.39171708, -2925.80632748])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 11 Dec 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:02:37</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Tue, 11 Dec 2018   Prob (F-statistic):               0.00\n",
       "Time:                        16:02:37   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = ss.OLS(df['price'], df[['intercept', 'area', 'bathrooms', 'bedrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p-values\n",
    "The p-values are the probabilites of $b_i$ to be 0. This actually shows us the \"usefullness of these parameters. In this case we see that area is a good predictor, while others are not.  \n",
    "\n",
    "**Significant bivariate relationships are not always significant in multiple linear regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared - the amount of variability in the response (y) explained by the model. Closer to 1 - better fit. In fact, R-squared is $r^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
